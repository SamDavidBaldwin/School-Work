Algorithm Structure: 
    The structure of the algorithm is based off of the stop_and_wait algorithm insofar as the algorithm waits for lsot packets until it recieves them. Unique from the stop_and_wait algorithm, instead of instantly writing to an outfile upon recieving packets, the algorithm instead saves packets in order, based on a higher level of data being saved when the packets are created. When they are created, packets hold half the text data, but also include information on which packet in order they are, the sum of the bytes of the text segment of the packet, and the result of 2 modulus calculations on that size. Each of these are checked on arrival of the packet to make sure there was no corruption, and then the algorithm sorts the packet into a pre-existing list based on the second data field in the packet. This helps ensure that if packets arrive out of order, the program doesn't print them out of order. The same is handled for duplicate packets. Because the packets have the identifier of which packet in order they are, it is easy to track duplicate packets. In the case of duplicates, the packet is still stored in the list, just behind the initial packet. Duplicate packets are checked against each other to make sure neither packet is corrupted. If one is, both are removed, and the packet is re-requested. 
    After all of the packets have been recieved, and the list is full, the program will recieve the end message, after which it will print the contents of the first index of the packet in each index of the list, outputing the whole text document to the outfile. The final response is then sent to the client and the copy is complete. 

Overview: In the overview, I will make note of parts of our algorithm which are different from sw. The underlying algorithmic structure is the same. 
    Chunk Creation --> Chunks are half the size of the sw algorithm, meaning there are two times the total packets. Chunks contain a list containing a string, an int tracking which packet in order it is, and 3 check fields. 
    Sending Chunks --> Right before sending, each chunk is put through pickle.dumps(chunk), turning the list object into a byte object that can be send, and then decoded once recieved. 
    Recieving Chunks --> Chunks are unpickled with pickle.loads(). The first data field in each recieved chunk is it's place in the order, and so the algorithm checks if the corresponding spot in the list is empty. If it is empty, the chunk is stored within the list. If the corresponding spot is not empty, the two lists are checked against each other. If they are the same, the newly recieved packet is stored behind the existing one. If not, both are cleared. Once the process of filling that list index is complete, an ACK is sent to show that the packet has been recieved. 
    Writing to Outfile --> Rather than writing each chunk of data to the outfile when it is recieved, the program only writes once, after the ///END\\\\\\\ chunk has been sent. This occurs once the list is entirely full, meaning there is no more data to be sent. Therefore, to write all the data in order, the algorithm iterates through the list, printing the first field of each element in the list, which is the text segment of each individual chunk. This results in the entire text segment being written to the outfile. 
    Double Send --> In an effor to reduce the possibility of corrupt packets impacting a read or other possible issues, packets are intentionally sent twice to ensure the lack of change of error. This reduces the possibility of error from 5 times the total corruption rate, to 5 times the corruption rate squared, an factor of roughly 100x in most of the scenarios presented. 
    

Algorithm Performance:
    Because of the multiple checks and the increased data being sent by our algorithm, it is slightly slower. It trades this speed for security, in that the algorithm will guaranteedly complete and return an accurate answer, it just requires a larger number of checks than the initial stop_and_wait algorithm which had little to no checking. Additionally, as a mechanism of sending lists over the socket, we utilized the pickle library, which over the course of a large number of packets, does slow the algorithm down. However, using pickle is an excellent check against corruption, as if a packet is corrupt in some way, the unpacking process will throw an error, making detecting corrupt packets incredibly easy and efficient. Ultimately, it appears that the algorithm we wrote is noticeably slower with lower error rates, packet corruption, and out of order packets but is noticeably faster and more accurate when these factors are increased. 

Additional Note: The algorithm at default conditions will fail in the terrible conditions as a consequence of consecutive fails. If the fail threshold is increased (as it is in the current iteration), it will not fail as commonly. Statistically, roughly 11 trials are required for the probability of a failure to be statistically insignificant. 
