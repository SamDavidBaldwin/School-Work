{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10 – Classification\n",
    "\n",
    "DUE: March 20, 2024 (Wed)  \n",
    "Pair Programming: Optional, but highly encouraged  \n",
    "Name(s): ???  \n",
    "Class: CSCI 349 - Intro to Data Mining  \n",
    "Section: ???   \n",
    "Semester: Spring 2024  \n",
    "Instructors: Brian King, Joshua Stough "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Objectives\n",
    "-   More visualization and EDA for classification problems\n",
    "-   Building and evaluating models with sklearn\n",
    "\n",
    "# Introduction\n",
    "This is your introduction to building classification models using scikit-learn (sklearn), and equally important evaluating them for performance assessment. There are also more visual EDA exercises to strengthen your plotting skills.\n",
    "\n",
    "You WILL be importing numerous *additional* packages beyond what we've given you as you work through the lab. You won't be told what to import. You should be able to figure that out using [insert your favorite AI assistant here] and good ol' fashioned Google. \n",
    "\n",
    "Generally, we recommend keeping all of your import statements neat and organized in the top cell of your notebook file. Though, if you want to keep this as a reference (which you should), it wouldn't hurt you to include it in the top cell AND also in the cell where you are answering the question.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# We used plotly. If you don't use plotly, then comment or remove these imports\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)**\t[P] Read about the [Iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set). This is perhaps the most commonly used dataset to teach students how to build classification models. It's a small dataset, and it's easy to understand. It's also a great dataset to learn how to visualize and explore your data. It's boring, but it's a great place to start.\n",
    "\n",
    "Let's show you the list of datasets built into Seaborn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.get_dataset_names()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You should see 'iris' in the list. Now, load iris into a dataframe called df_iris. Show the info() output to confirm that you have a dataframe with 150 observations and 5 variables."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)**\t[P] Be thankful for a moment, because the data are clean. However, the `species` variable needs work. Convert the variable to a pandas `Categorical` variable. Then show the distribution of your variable (how many of each species?). Repeat the `info()` output to show that your target variable is now categorical, and not merely an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)**\t[P] Now, perform essential summarizing tasks on your data. Show the output of describe() and show the first 10 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## More visual EDA and plotting\n",
    "\n",
    "It's far more telling to generate good visualizations of your data. You will generate numerous plots of your data. Use the interface of your choosing. Any of matplotlib, seaborn, plotly, or a combination of all, are acceptable. (HINT - Sometimes for quick visualizations, the pandas interface to matplotlib is simple, effective and the way to go! For example, `df_iris.boxplot()`! )\n",
    "\n",
    "> Do we need to normalize these data for visualizations? All four variables are measurements on the same scale. It's actually more telling to ensure they are on the original scale and not normalized in any way. Remember – a data scientist has a wealth of tools and tricks to accomplish a large number of tasks to get from raw data to models, predictions, and visualizations. Don't ever start blindly applying random steps! That will always lead you to dangerous territory in data science, possibly causing you to draw conclusions that will only get you in trouble.\n",
    "\n",
    "**For these plots, minimally, be sure to title and label your plots!**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4)**\t[P] Always start with basic univariate plots. However, let's show the four variables as boxplots, side by side. Create a single boxplot showing each of the four independent variables on one plot. Ignore the target variable species for the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "With all plots requested, you may use your own color schemes and be creative.\n",
    "\n",
    "## \"Melting\" your data into a long format\n",
    "\n",
    "Seaborn is great for dealing with lots of variables. And even better, it usually knows how to interpret those variables… usually. But its interface is limited for interactive data visualization. If you're using **Plotly**, you can make the above graph using the lower level graph_objects interface, and use the add_trace method of the Figure object to add subsequent box plots for each variable. However, this approach, while powerful, requires several lines of code. Sometimes you want the quickness and ease of plotly.express to create many very simple graphs. To do this, **you need to transform your data into a melted frame using pd.melt()**. This cool function quickly transforms your data frame from a \"wide\" into \"long\" format, making it very easy to still use plotly.express. In fact, Plotly Express explicitly suggests that you transform your data into what they call **Tidy Data** using the pd.melt() function (See <https://plot.ly/python/px-arguments/> ) Or, you can just use the melt method of your DataFrame object. Both will accomplish the same thing. It's worth taking a moment read about this valuable pandas function. It can help you with many visualization methods in both Seaborn, Plotly and others.\n",
    "\n",
    "For example, try this one out:\n",
    "```\n",
    "df_iris_melt = pd.melt(df_iris, id_vars=\"species\")\n",
    "display(df_iris_melt.head(5))\n",
    "```\n",
    "Notice how it changed the structure of your dataframe? Now, you can easily use that in with the simpler Plotly Express functions. For example, try this in another cell:\n",
    "\n",
    "```\n",
    "fig = px.box(df_iris_melt, x='variable',y='value',\n",
    "             title=\"Fisher's Iris Data\")\n",
    "fig.update_yaxes(title=\"cm\")\n",
    "fig.show()\n",
    "```\n",
    "\n",
    "There is a learning curve with plotly, but entirely worth your time. It's a powerful tool for interactive data visualization. And, it's a great way to make your data visualizations interactive and web-ready.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5)**\t[M/P] 5)\t[M/P] Violin plots are becoming increasingly common in data science. First, explain what a violin plot is. Then, figure out how to generate a univariate violin plot of each independent variable. Compare and contrast your violin plot against the boxplot. (NOTE: Seaborn makes this very easy!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T21:33:37.861346Z",
     "start_time": "2024-03-05T21:33:37.859432Z"
    }
   },
   "outputs": [],
   "source": [
    "# ANSWER:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "ANSWER: \n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6)**\t[P] Show a histogram and/or a density plot of each variable on a single plot. And be sure to provide some way to see the distribution of all four variables separately. You could use alpha blending on the histogram, or perhaps consider a \"rugplot\" overlaid on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7)**\t[M] Summarize your findings from these plots. Is one most ideal for understanding your distribution? Characterize the distribution of your four variables. Remember, this is a univariate exploration, so you don't care about the class variable yet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "ANSWER: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "Now, you will perform some basic multivariate analyses. If you're NOT using plotly, be sure to explore how to make plots large enough to examine using the figsize parameter. A reasonable size should be (10,8)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8)**\t[P] Use the pandas interface to generate a quick boxplot (df_iris.boxplot()!)  However, look up how to created a faceted boxplot with each variable listed in a separate plot, automatically showing the distributions of your variables by \"species\". \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9)**\t[P] Use either seaborn or plotly to generate a boxplot over each variable, but now showing the three different species as distinct boxplots. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10)**\t[P] Generate a violin plot over all variables much like the previous plot, but again, be sure to indicate the species as a distinct color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11)**\t [P] Read about the `scatter_matrix()` function in pandas. Use it to generate a scatterplot matrix, and use species for the color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER: \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12)**\t[P] Read about the seaborn `pairplot()` function, then use it to generate one of the most useful scatterplots matrices you'll see with this data. It won't be too different than the scatter_matrix, just easier to create. Take a moment to study the plot, and really try to understand just how much information this plot is conveying. Be sure to figure out how to distinguish the species by color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**13)**\t [M] From your observations, which species do you expect to have the best classifier performance? Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**setosa** should be the clear winner. It is easily distinguished from the other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Preparing for scikit-learn\n",
    "\n",
    "OK. Let's get started with our first adventure into machine learning using scikit-learn. So far, you've learned a few different methods for modeling data. The majority of this lab will be designed to get you going with creating models, but there will be more emphasis on evaluation (performance on your training data) and validation (performance on your test data) of your models.  Follow along, and as usual, you are encouraged to take extra notes in your markdown file as these labs will make great references for you. Make the most of this lab! There is a lot here, and it represents the typical workflow of those who actually work with these types of data modeling problems using machine learning in practice.  The next lab will give you more opportunities to explore sklearn with more challenging data. However, you need to learn the API with the basics first.\n",
    "\n",
    "> We will start simple, but will progressively, and quickly, release you to use your own resrouces to guide your own learning to find the answers.\n",
    "\n",
    "Before you begin, read through the scikit tutorial's first section that introduces the user to machine learning with scikit-learn. It's mostly review, but will have some information and examples that could be quite helpful. Keep the tutorial and user guide handy for the remainder of the semester. You will surely need it!\n",
    "\n",
    "**Do NOT use the iris data built into scikit-learn! Just keep working with the data you have already loaded in above.**\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Preparing our data for modeling\n",
    "\n",
    "When you are done with preprocessing, you will have a data frame that contains all of your data ready to model. Even before you split your data into training and test data, most modeling methods will require you to separate the independent (predictor) variables and the dependent (target) variable(s). The convention generally followed is to use the name **`X`** to represent the data frame for your independent variables, and the target variable is denoted **`y`**. Both should be data frames, though if `y` is a single variable (most common), then it's OK to represent it as a `pd.Series` or even a `np.array` object.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**14)**\t[P] Split your data frame into X and y, where X represents only your four predictor variables, and y represents only the target class, species. Output the names of the columns and the shape of both just to confirm that they both have the same number of observations, and that the number of variables in each are correct. You should have (150, 4), and (150, 1) respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Building our classifier\n",
    "\n",
    "We're going to dive right in and build a classifier using default hyperparameters, using ALL of our data for training data. Since we started with the decision tree model, let's begin there. The model we want to begin with is the first model we learned about in class – the DecisionTreeClassifier. You can read about it here: <https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html>   \n",
    "  \n",
    "Generally, the sklearn documentation uses a standard convention naming all classifier objects as clf. All classifiers have a common interface: you use fit() to train a model with training data, and predict() to generate predictions on any data it is given. A common interface like this makes is incredibly easy to try out different models on your data.\n",
    "\n",
    "**NOTE: You will need to be sure you import the proper libraries from sklearn! You will not be told which packages to import here.**\n",
    "\n",
    "NOTE: Remember the standard machine learning pipeline:  \n",
    "  \n",
    "1) Clean your data  \n",
    "2) Build your model  \n",
    "3) Evaluation – Evaluate model performance on training data (same data used to build the model. If your performance on training data is bad, there is no point in continuing to validation.)  \n",
    "4) Validation – Validate the model by assessing performance on test data (i.e. different data than training data  \n",
    "  \n",
    "Whenever I work with a new classifier, I always try to ensure the classifier can be trained with ALL of my data, and then predict back that same data. Don't bother jumping into validation of your model with cross-validation until you know the classifier can at least do well predicting the data that you trained it with! If it can't even do that, then something has gone horribly wrong.\n",
    "\n",
    "Let's build a classifier with all of the data and evaluate its performance. For the time, let's ignore normalizing the attributes, and leave our data as is. As we discussed in the beginning, all four variables are mostly falling within a similar range, representing the same unit, and thus, at least for these data, normalizing is not as critical yet, particularly since we are going to start working with decision trees.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**15)**\t[P] Create an instance of a decision tree classifier using `DecisionTreeCassifier()` with default parameters.  Name the classifier `clf`. Train the classifier with the entire dataset (i.e. all of X and y.) Show the classifier after training by simply include `print(clf)` so you can see the default parameters used to build the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**16)**\tP] Remember that one of the most popular reasons for using decision trees is because the model is easily visualized for model interpretation purposes. Use the plot_tree method to plot the tree. Explore the arguments to be sure that nodes are shaded by target class. Feature and class names should be shown. You should notice that the class you expected to perform well due to a clear decision boundary noted in your EDA above is indeed perfectly classified with only one node off the root node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**17)**\t[P] - Use this model to predict back your training data to evaluate your model. Name your predictions `y_pred`. Report the score using the score method on the classifier. You should see 100% accuracy. Why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**18)**\t[M] You should see 100% accuracy. Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**19)**\t[P] Let's simplify our tree. Create a new tree, but adjust the pruning / complexity parameters. How? We'll keep this simple. Ensure that every leaf in the tree contains at least 5 samples. Show the tree that you induced, and again store your predictions as y_pred. Then, reveal the accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**20)**\t[M/P] OK – clearly we have lower accuracy. Dive deeper. Accuracy is usually not a good measure of classifier performance. Look up the function `classification_report`. This outputs a lot of performance information! What do the micro, macro, and weighted average readings mean? Generate a full report. Which class had the lowest precision? Which had the lowest recall?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "> (You are strongly encouraged (not required) to delve into understanding what the micro, macro, and weighted average readings mean. Take notes in this section if you are inclined to do so. This function will do a LOT for you!)\n",
    "\n",
    "> Which class had the lowest precision? Which had the lowest recall?\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**21)**\t[P] Output a confusion matrix using the `confusion_matrix` method in `sklearn.metrics`. Your result should look like a square matrix, where rows are the true labels, and the columns are the predicted labels, and the diagonal represents the cases where the true label and predicted label match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**22)**\t[P] Even better! Look up how to use `ConfusionMatrixDisplay` from `sklearn.metrics` to output an excellent visual summary of the classifier performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ANSWER:\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**23)**\t[M] Interpret your confusion matrix and classification report. Which class is performing the best? Which is performing the worst? How many total incorrect predictions?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Evaluation vs. Validation\n",
    "\n",
    "Up to now you've only really evaluated your model, where you build a model, and use it to predict back your training data. Once you have good results with training data, it's time to partition our data into training and testing datasets. Then, using the test data, we can validate our model to determine that it indeed is not overfitting our model.\n",
    "\n",
    "**Cross validation** is the most common technique used to validate a machine learned model. There are many different approaches to validation (which has been, or will soon be covered in lecture!) We're going to start pretty simple. Fortunately, scikit-learn has plenty of useful functions to assist with the evaluation and validation of your model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**24)**\tYou need to create a train / test split of your data to properly validate your model. Read about `sklearn.model_selection` function called `train_test_split`. Then, use it to split your data into an 70% / 30% split of training and testing data, respectively. You should end up with four data frames, denoted X_train, X_test, Y_train, Y_test. Use an initial seed of 0. Be sure to shuffle the data (verify that this is a default setting.) Show the dimensions of these (i.e. how many entries in each?)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "NOTE: Technically, `train_test_split`, like most scikit-learn modules, ignore pandas, and return numpy arrays. If you properly set up your data, and maintain your types throughout, it will do a reasonable job maintaining those types. I generally find it's important to keep my data, and splits, my predicted values, etc. all as pandas DataFrames, but this is a matter of preference. If you are missing your column and index variables, it's most likely because you lost type information at some point in your pipeline. Just cast your data back to a pandas DataFrame and restore those important variables."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**25)**\tCreate a new instance of `DecisionTreeClassifier` with an initial random seed value of 100, and a minimum number of samples in a leaf set to 5. Store the classifier as `clf` again. Train your classifier with your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**26)**\tUse this model to predict the labels on your training and test data. Call your predictions y_pred_train. and y_pred_test. Show the accuracy on both your training data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**27)**\t[P/M] Use the classification report and confusion matrix techniques discussed above to assess the performance of your classifier on both the training and the test data. Summarize your findings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**28)**\t [P] Quite often, when we have misclassifications, it's important to take the time to dig into your test data to determine which observations are being misclassified. Use data selection techniques to output the data that are being misclassified in the test data only.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**29)** [P] It's even more interesting when you can visualize where in your feature space your classifier might be missing something. Create a scatterplot matrix, but this time, highlight the instance(s) that are being misclassified. \n",
    "\n",
    "> (HOW? Can you create a new variable that represents the accuracy of the prediction? Use it as another dimension? A new color? A larger point size? Any scatter plot you can come up with that will highlight the location of the misclassified data receives full credit. This is an important step in understanding how to deal with problematic data.)\n",
    "\n",
    "If you do this in a useful way, you'll get a nice visual indicator that, as expected, the misclassified instances were instances falling on the divide between versicolor and virginicia \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**30)**\t[M/P] Read about the `KFold` class. State what a `KFold` cross validation object will do for you. Then, create an instance of KFold with 10 splits, an initial seed of 100, and be sure to shuffle your data. Call your model kfold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**31)**\t[P] Read about the `split()` method for `KFold`. It will help you iterate through each fold of your data. Take the time to understand how it works! If you set this up right, you will have a loop that will let you easily select train and test data for each fold. Once you do that, then you are merely repeated what you did already for the 70/30 split data above! For each fold, you fit a new model on the training data for that fold, then validate the model on the test data for the same fold. After 10 folds, you will have validated your model using your entire dataset as validation data!\n",
    "\n",
    "Inside your loop, you should NOT print out your classification results. (However, sometimes it's useful to use Python timing functions to collect timing on different model building tasks, and that would be done inside the loop, placing a time stamp before and after the call to fit().) Only store the results of predicting on the test fold. I would accumulate my true class label and my predictions in a separate data frame, called df_results.\n",
    "\n",
    "If you do this right, you will have a new data frame with a variable, such as \"dt_def\", or \"dt_default\", representing the prediction from the default decision tree classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**32)**\t[P] Print out a classification report from your 10 fold cross validation. Also print out your confusion matrix with a plot from `ConfusionMatrixDisplay` as you have done above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ANSWER:\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**33)**\t [P] Select and generate a report all of your test instances that were misclassified from the 10 fold cross validation. Also, generate a scatterplot that highlights the location of the instances that were misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**34)** \t[P] As you would expect, the scikit-learn framework has some powerful methods that can run an entire cross validation and report whatever metrics you want. Read about the cross_validate method, then use it to run a 10-fold cross validation on a default decision tree, reporting back 'accuracy' and 'f1_macro' measurements on both the training and testing data. Report your results as a single data frame. Your results should look as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**35)** [M] On the above, what are the variables fit_time and score_time?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**36)**\t[P] OK, one last function for validation purposes. Read about the function cross_val_predict. This is perhaps among the most powerful of the model selection functions provided by sklearn, as it pretty much does everything we need in one function call. And, the predictions are returned! You can then use these predictions to run a classification_report and report confusion matrices.  Use the cross_val_predict function to run a 10-fold cross validation with a default decision tree, and print the classification_report on your results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**37)** [MP] For your last task, you will perform model comparison tasks. Use the cross_val_predict method to compare the predictive performance on the following models:\n",
    "    a.  A default decision tree\n",
    "    b.  A decision tree with \"entropy\" for measuring impurity\n",
    "    c.  A `KNeighborsClassifier` with a two different values of k (.)\n",
    "    d.  `MultinomialNB` classifier (sklearn's Naïve Bayes implementation)\n",
    "\n",
    "Compare and contrast the performance results between the different models. Which one would you choose?\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER - default decision tree\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER - A decision tree with \"entropy\" for measuring impurity\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER - A KNeighborsClassifier with a k = ???\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER - A KNeighborsClassifier with a k = ???\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER - A MultinomialNB classifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "ANSWER:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
